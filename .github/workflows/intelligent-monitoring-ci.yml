name: Intelligent Monitoring Framework CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *' # Daily at 2 AM UTC
  workflow_dispatch:
    inputs:
      run_long_term_tests:
        description: 'Run long-term ML tests'
        required: false
        default: 'true'
        type: boolean
      error_tolerance_mode:
        description: 'Continue on non-severe errors'
        required: false
        default: 'true'
        type: boolean

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.11'
  POSTGRES_DB: imf_test
  POSTGRES_USER: postgres
  POSTGRES_PASSWORD: postgres
  DATABASE_URL: postgresql://postgres:postgres@localhost:5432/imf_test

jobs:
  # Priority 1: Long-term ML Tests (Critical - must pass)
  long-term-ml-tests:
    name: 🧠 Long-term ML & Continuous Learning
    runs-on: ubuntu-latest
    timeout-minutes: 45
    continue-on-error: false
    outputs:
      ml-status: ${{ steps.longterm_tests.outcome }}
      test-summary: ${{ steps.test_summary.outputs.summary }}
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: 📥 Checkout
        uses: actions/checkout@v4

      - name: 🟢 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: 📦 Install dependencies
        run: |
          npm ci
          cd python-framework && pip install -r requirements.txt

      - name: 🗃️ Setup database
        run: npm run db:migrate
        env:
          DATABASE_URL: ${{ env.DATABASE_URL }}

      - name: 🎯 Run Long-term ML Tests
        id: longterm_tests
        run: npm test -- --run server/test/long-term/mcp-monitoring-continuous-learning-simple.test.ts
        env:
          DATABASE_URL: ${{ env.DATABASE_URL }}
          PYTHON_PATH: python3

      - name: 📊 Generate test summary
        id: test_summary
        if: always()
        run: |
          if [ "${{ steps.longterm_tests.outcome }}" == "success" ]; then
            echo "summary=✅ ML tests passed - Progressive learning validated" >> $GITHUB_OUTPUT
          else
            echo "summary=❌ ML tests failed - Critical for deployment" >> $GITHUB_OUTPUT
          fi

      - name: 📈 Upload ML artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ml-test-results
          path: |
            test-results/
            python-framework/ai_models/
          retention-days: 30

  # Standard Tests (Error tolerant)
  standard-tests:
    name: 🧪 Standard Tests (${{ matrix.test-group }})
    runs-on: ubuntu-latest
    timeout-minutes: 25
    continue-on-error: ${{ github.event.inputs.error_tolerance_mode != 'false' }}
    needs: long-term-ml-tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5
        ports:
          - 5432:5432

    strategy:
      fail-fast: false
      matrix:
        test-group: [unit, integration, mcp-api, intelligent-monitoring]

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: 📦 Install dependencies
        run: |
          npm ci
          cd python-framework && pip install -r requirements.txt

      - name: 🗃️ Setup database
        run: npm run db:migrate
        env:
          DATABASE_URL: ${{ env.DATABASE_URL }}

      - name: 🧪 Run ${{ matrix.test-group }} tests
        id: test_group
        continue-on-error: true
        run: |
          case "${{ matrix.test-group }}" in
            unit)
              npm test -- --run "server/test/!(long-term|intelligent-mcp-monitoring)/**/*.test.ts"
              ;;
            integration)
              npm test -- --run "server/test/integration/*.test.ts"
              ;;
            mcp-api)
              npm test -- --run "server/test/mcp-api.test.ts"
              ;;
            intelligent-monitoring)
              npm test -- --run "server/test/intelligent-mcp-monitoring.test.ts"
              ;;
          esac
        env:
          DATABASE_URL: ${{ env.DATABASE_URL }}
          PYTHON_PATH: python3

      - name: 📊 Log results
        if: always()
        run: |
          if [ "${{ steps.test_group.outcome }}" == "success" ]; then
            echo "✅ ${{ matrix.test-group }} tests passed"
          else
            echo "⚠️ ${{ matrix.test-group }} tests failed (non-blocking in error-tolerant mode)"
          fi

  # Code Quality (Non-blocking)
  code-quality:
    name: 📝 Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 10
    continue-on-error: true
    needs: long-term-ml-tests

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: 📦 Install dependencies
        run: npm ci

      - name: 🔍 ESLint
        continue-on-error: true
        run: npm run lint || echo "⚠️ ESLint issues (non-blocking)"

      - name: 🎨 Prettier
        continue-on-error: true
        run: npm run format:check || echo "⚠️ Format issues (non-blocking)"

      - name: 📏 TypeScript
        continue-on-error: true
        run: npm run typecheck || echo "⚠️ Type issues (non-blocking)"

  # Build & Deployment (Critical)
  build-validation:
    name: 🏗️ Build & Deploy Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    continue-on-error: false
    needs: [long-term-ml-tests]
    if: always() && needs.long-term-ml-tests.result == 'success'

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: 📦 Install dependencies
        run: |
          npm ci
          cd python-framework && pip install -r requirements.txt

      - name: 🏗️ Build
        run: npm run build

      - name: 🎬 Test ML demo
        run: |
          cd python-framework
          timeout 30s python3 demo_mcp_monitoring.py || echo "Demo completed"

      - name: ✅ Ready for deployment
        run: echo "Build validation passed - ready for deployment"

  # Final Status
  final-status:
    name: 📊 CI/CD Status Report
    runs-on: ubuntu-latest
    needs: [long-term-ml-tests, standard-tests, code-quality, build-validation]
    if: always()

    steps:
      - name: 📊 Status report
        run: |
          echo "🎯 IMF CI/CD REPORT"
          echo "=================="
          echo "🏆 Long-term ML: ${{ needs.long-term-ml-tests.result }}"
          echo "🧪 Standard Tests: ${{ needs.standard-tests.result }}"
          echo "📝 Code Quality: ${{ needs.code-quality.result }}"
          echo "🏗️ Build: ${{ needs.build-validation.result }}"
          echo ""
          
          ML_OK="${{ needs.long-term-ml-tests.result }}"
          BUILD_OK="${{ needs.build-validation.result }}"
          
          if [ "$ML_OK" == "success" ] && [ "$BUILD_OK" == "success" ]; then
            echo "🎉 STATUS: ✅ READY FOR DEPLOYMENT"
            echo "   - Priority 1 ML tests passed ✅"
            echo "   - Build validation successful ✅"
          elif [ "$ML_OK" != "success" ]; then
            echo "❌ STATUS: 🚫 BLOCKED (ML tests failed)"
          elif [ "$BUILD_OK" != "success" ]; then
            echo "❌ STATUS: 🚫 BLOCKED (Build failed)"
          else
            echo "⚠️ STATUS: 🚫 BLOCKED (Unknown issue)"
          fi
          
          echo ""
          echo "📈 Error tolerance: ${{ github.event.inputs.error_tolerance_mode || 'enabled' }}"

      - name: 🎯 Set exit code
        run: |
          ML_OK="${{ needs.long-term-ml-tests.result }}"
          BUILD_OK="${{ needs.build-validation.result }}"
          
          if [ "$ML_OK" == "success" ] && [ "$BUILD_OK" == "success" ]; then
            echo "✅ Pipeline successful"
            exit 0
          else
            echo "❌ Pipeline failed"
            exit 1
          fi